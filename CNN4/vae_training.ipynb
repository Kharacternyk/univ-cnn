{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd7be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup  # keep this\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "c_ = Fore.CYAN\n",
    "g_ = Fore.GREEN\n",
    "y_ = Fore.YELLOW\n",
    "m_ = Fore.MAGENTA\n",
    "sr_ = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a72bac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, test_sample, figsize=(20,15)):\n",
    "    f, axarr = plt.subplots(1,2,figsize=figsize)\n",
    "    img= torchvision.utils.make_grid(test_sample, normalize=True).permute(1,2,0).numpy()\n",
    "    axarr[0].imshow(img)\n",
    "    \n",
    "    # mean, logvar = model.encode(test_sample)\n",
    "    # std = torch.exp(logvar/2)\n",
    "    # z = model.reparamatrize(mean, std)\n",
    "    # predictions = model.decode(z).detach().cpu()\n",
    "\n",
    "    # predictions, _ = model(test_sample)\n",
    "    mu, _ = model.encode(test_sample)\n",
    "    z = torch.randn_like(mu).to(test_sample.device)\n",
    "\n",
    "    predictions = model.decode(z)\n",
    "    predictions = predictions.detach().cpu()\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    img = torchvision.utils.make_grid(predictions, normalize=True).permute(1,2,0).numpy()\n",
    "\n",
    "    plt.savefig('image.png')\n",
    "    axarr[1].imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5df2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae_training import VAE\n",
    "\n",
    "# model = Model()\n",
    "# model.load_state_dict(torch.load('./imagenet_vae_model.bin'))\n",
    "# model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "model = VAE(latent_dim=512, img_size=128, beta=1.0).to(device)\n",
    "model.load_state_dict(torch.load('C:/Users/pzaka/Documents/CNN/imagenet_vae_model.bin', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "config = {'lr':1e-3,\n",
    "        'wd':1e-2,\n",
    "        'bs':256,\n",
    "        'img_size':128,\n",
    "        'epochs':100,\n",
    "        'seed':1000}\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((config['img_size'],config['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                        std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "valid_dataset = datasets.ImageFolder(root=os.path.join(\"C:/Users/pzaka/Documents/datasets/imagewoof2\", \"val\"), transform=train_transform)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "dataiter = iter(valid_dl)\n",
    "sample = next(dataiter)\n",
    "\n",
    "generate_and_save_images(model,sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ded577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n",
    "vae.requires_grad_(False)\n",
    "vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f51d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(valid_dl)\n",
    "test_sample = next(dataiter)[0]\n",
    "\n",
    "f, axarr = plt.subplots(1,2,figsize=(20,15))\n",
    "img= torchvision.utils.make_grid(test_sample, normalize=True).permute(1,2,0).numpy()\n",
    "axarr[0].imshow(img)\n",
    "\n",
    "\n",
    "predictions = vae.encode(test_sample).latent_dist.sample()\n",
    "\n",
    "# predictions = torch.randn((test_sample.shape[0], 4, 32, 32)).to(device)\n",
    "predictions = vae.decode(predictions).sample\n",
    "predictions = predictions.detach().cpu()\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "img = torchvision.utils.make_grid(predictions, normalize=True).permute(1,2,0).numpy()\n",
    "axarr[1].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "latent = []\n",
    "for v, l in tqdm.tqdm(valid_dl):\n",
    "    if l == 0:\n",
    "        latent.append(vae.encode(v).latent_dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a297d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.randn(1, 4, 16, 16) * torch.stack(latent[:2]).std(0) +  torch.stack(latent[:2]).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e539f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = vae.decode(latent[0]).sample\n",
    "predictions2 = vae.decode(latent[5]).sample\n",
    "\n",
    "predictions = vae.decode((latent[0] + latent[5]) / 2).sample\n",
    "\n",
    "f, axarr = plt.subplots(1,3)\n",
    "img = torchvision.utils.make_grid(predictions1, normalize=True).permute(1,2,0).numpy()\n",
    "axarr[0].imshow(img)\n",
    "img = torchvision.utils.make_grid(predictions2, normalize=True).permute(1,2,0).numpy()\n",
    "axarr[1].imshow(img)\n",
    "img = torchvision.utils.make_grid(predictions, normalize=True).permute(1,2,0).numpy()\n",
    "axarr[2].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(torch.stack(latent).cpu()[:,0,0,0,0].numpy(), torch.stack(latent).cpu()[:,0,0,0,1].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(torch.stack(latent).cpu()[:,0,0,0,0].numpy(), bins=30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset_distilation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
